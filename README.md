# Clustering Analysis of Knee Osteoarthritis Structural Progression Profiles
## Motivation and rationale  

### The Context:  
Osteoarthritis is a common condition which affects joints creating pain and difficulty in moving, in this I will be focusing on occurrences within the knee joint. It is caused by cartilage breaking down within the joints which can occur from injuries, family history and obesity [1]. Osteo is the most common form of arthritis affecting over 32.5 million adults in the US alone [2]. It is a common condition in the older population because it occurs due to usage and so increasing age means an increased risked of osteoarthritis.  

### The Problem:  
Many subtypes of diseases are present within the osteoarthritis cohort but knowing distinct subtypes could further our understanding of the disease. With this furthered understanding we can realise cases where people may be more predisposed to osteoarthritis than initially expected. As well as this we can improve treatment plans to be more relevant to specific groups and narrow research areas. The main difficulty will be clustering the data from the progression profiles as there are many different algorithms all with different variations and parameter options. So I will need to measure the success of the clusters produced.

### My Approach:
For my project I intend to analyse patient progression profiles from a knee osteoarthritis cohort  and find similarities in these profiles, showing distinct subtypes of diseases. To do this I will unify the data from these profiles doing and use a clustering algorithm to group data by similarities. These clusters will show the mentioned subtypes of diseases and how common they are. To make this information understandable I will produce visualisations of the findings from this.  
I aim to do this with the use of Python and its packages Pandas, Scikit-learn and Seaborn. Which are for data analysis, clustering algorithms and visualisation respectively.

## Aim and objectives  
### Aim: Analyse data from patient progression profiles to find common subtypes of diseases and visualise the findings
### Objectives:
1)	Understand the data present in the progression profiles  
Improving my understanding of what the data in these profiles represents and means will allow me to refine my research areas and ability to analyse the data. I will complete this objective before I begin to unify the data as otherwise mistakes will likely occur.  
2)	Unify the data from progression profiles  
Combining all of the data from the profiles means I can begin implementing a clustering algorithm. It will also give me a new view of the data from which I can hopefully begin to draw some conclusions and predictions of what the clusters may resemble when produced, enabling me to make informed decisions when assessing the clusters.  
3)	Explore different clustering algorithms that can be used with the data  
These algorithms will group similar data together showing common occurrences between the progression profiles and trends in the data. I will visualise these clusters because the clustering algorithms have various parameters each effecting the clusters produced and in turn the provided view, so I need to know how they differ visually from one another.  
4)	Group common similarities between profiles using optimal clustering algorithm  
I will discover the optimal clustering algorithm using metrics built-in to scikit-learn [4]. For example, using “Silhouette Coefficient” which produces a numerical score representing the distance between clusters, where the aim is to achieve a high score. I will be using three separate metrics, which will allow me to assess the clustering algorithms and tune them as necessary.  
5)	Visualise the similarities in an informative manner  
With the data being clustered I can begin to draw conclusions from the groupings by producing visualisations of the data. These visualisations should show distinct patient subgroups.  
## Background Research
| Source                                            | Description | Relevance |
|---------------------------------------------------|-------------|----------------------------------------------------------------------------|
| Osteoarthritis: toward a comprehensive understanding of pathological mechanism [4] | This paper provides a lot of background information on Osteoarthritis, including Its effects and causes. It outlines areas such as progression of the disease as being poorly understood. As a result of the lack of understanding understanding there’s no method to deaccelerate the progression of the disease.| Reading the paper conveys the seriousness of the disease and further solidifies how common it is. Learning about the disease will greatly affect my working as well as my effectiveness in understanding and analysing the data. This paper also shows the need for further research on this topic. |
| Patients with knee osteoarthritis can be divided into subgroups [5] | This article shows an example where the osteoarthritis cohort can be divided into subgroups using a clustering algorithm. These subgroups showed the patients could be divided by specific clinical characteristics and how these findings can further the understanding of development, potentially inspiring improved treatment strategies. | This shows an example where clustering knee osteoarthritis cohort data can be used to further our understanding of the disease and highlight distinct groups. Furthermore, Python was used to produce clustering with K-means, the article explains how they implemented the algorithm, including the number of repetitions and how they found the number of subgroups to use. I am intending to use K-means so this methodology is very useful to learn about. |
| Types of Clustering Algorithms [6] | This discusses four types of clustering algorithms that can be used, all of which contain their own variants but the focus will be on these types included. Each variant produces very different visuals of clusters showing how different they are and the need to explore different types rather than variants. | Clustering will be a big part of the project, I will produce many variations of clusters using clustering algorithms all with various parameters. These are the most common algorithms and will be the focus of my experiments in clustering.  I am intending to use a few clustering types which are explored in this article, one of which is Density-Based clustering. This type is good in this scenario because the data does not have high dimensions and this algorithm does not assign outliers to clusters, which will prevent visualisations from being misleading. I can implement this type of algorithm with Scikit-learn in two ways which are through DBSCAN and OPTICS, which produce very similar results but the latter produces a reachability graph. This graph could be useful in assessing the effectiveness of the algorithm and also further my understanding of the results. |
| Clustering Algorithms: A Comparative Approach [7]  | This paper gives background information to clustering algorithms and assesses how common types perform. It shows how performance can vary between the types of data tested and the parameters of each algorithm type. It makes three assessments for each type tested, which are using the default parameters and single or random parameter changes. | I need to assess various types of clustering algorithms all with their own parameters, this provides ideas on how to vary the parameters for testing the algorithms quickly and uniformly. It also outlines some common clustering methods and gives some explanations about how they work and what their parameters represent.  One of the algorithms outlined in this article is hierarchical clustering which I am intending to use. It found this type performed well on smaller datasets, was the fastest tested and had limited performance on larger datasets. There are also two variations which are agglomerative and divisive which are top down and bottom-up approaches for the clusters, meaning the hierarchy will either start or end with a unique cluster. |
| What Makes a Visualization Memorable [8] | This discusses the importance of visualisations and how they can be easily insufficient in conveying their meaning. It shows measurements of memorability for various visualizations, showing how effective they are. | I will have to visualise my findings and clusters in a meaningful way, in which I mean one where the data can be easily understood. Each of the clustering algorithms will produce different visuals so I need to ensure that when presenting these findings, they stand out from one another. |

## Work plan  
### Done so far:
Firstly, I have done background research on a few of the topics I will be encountering throughout. The main one being based around clustering algorithms, these will be a big part of the project represented in objectives 3 and 4. I have read an article from Google Developers [6], which summarised some of the common types and visualised example results. From this article I concluded a distribution-based algorithm such as DBSCAN or OPTICS would be a good choice, which can be done with scikit-learn.  

Furthermore, I have looked at a paper discussing the performances of different types [7], which gave some in-depth comparisons and experiments of common clustering algorithms. This solidified some of my understanding produced from the prior article, as well as explaining some of the workings of the algorithms and their parameters. As a result, I am confident in the usage of hierarchal clustering and I will do some brief testing between divisive and agglomerative clustering.  

I will be using scikit-learn to implement the algorithms because this package enables me to implement the mentioned clustering algorithms and useful metrics to assess the clusters. ( will also be using Pandas and Seaborn as I have experience with both and they are well suited to the task, Pandas will be used for processing the data and preparing it for implementing the algorithms and Seaborn will be used for visualisations, I chose this package because it is designed around Pandas.


### Future Plans:
To start with I will solidify my understanding of the data. This will be crucial for the next steps because if I lack this understanding, I will be very prone to making mistakes which will delay my progress. Following this I will begin to integrate the data, which produces a new view of the data enabling me to draw some initial conclusions. 

At this point I will begin implementing the clustering algorithms and I will evaluate the following types: K-means, density-based and hierarchal. I will repeat each experiment although this number is not yet known as I will not know how long they will take until I have begun this phase. I will have to evaluate each algorithm with different parameter configurations. I intend to perform parameter sweeps for each of the algorithm types to find the optimal configuration. I will know if configurations are appropriate because the results will be stable and be similar to past results from other algorithm types. Each of these configurations will be evaluated with Silhouette Coefficient, Calinski-Harabasz Index and Davies-Bouldin Index. I will be using these metrics because they do not require the ground truth to be known. However, I will have to be aware each of these metrics tend to score higher for density-based clusters such.  

Finally, I will produce high quality visualisations of the final clusters, these visualisations will show which distinct patient subgroups exist within the Osteoarthritis Initiative cohort. This is important as I will likely have to produce multiple versions of the visualisations to ensure they are informative.  

### Risks and Mitigation
Each phase of the work is dependent on one another as shown in the Gantt chart, meaning if I fall behind on one task I lose time to complete all those that follow. The main risk would comes from the clustering algorithm tasks, this is because I will be performing many experiments but I will not know how long they will take until beginning them. However, I should be able to recover from these delays as weekends are unplanned time. Furthermore, experiments should become easier to setup as I progress through the phase and so I will be actively gaining time. Although if this issue does occur and I do not believe I will be able to makeup the lost time I can simply reduce the number of experiments.

## References  
[1] NHS (2019). Overview - Osteoarthritis. NHS. Available at: https://www.nhs.uk/conditions/osteoarthritis/.  
[2] CDC (2020). Osteoarthritis (OA).  Centers for Disease Control and Prevention. Available at: https://www.cdc.gov/arthritis/basics/osteoarthritis.htm.   
[3] Scikit-learn.org. (2010). 2.3. Clustering — scikit-learn 0.20.3 documentation.  Available at: https://scikit-learn.org/stable/modules/clustering.html.   
[4] Chen, D., Shen, J., Zhao, W., Wang, T., Han, L., Hamilton, J.L. and Im, H.-J. (2017). Osteoarthritis: toward a comprehensive understanding of pathological mechanism. Bone Research. Available at: https://doi.org/10.1038/boneres.2016.44   
[5] Petersen E.T., Rytter S., Koppens D., Dalsgaard J., Hansen T.B., Larsen N.E., Andersen M.S. and Stilling M. (2022). Patients with knee osteoarthritis can be divided into subgroups based on tibiofemoral joint kinematics of gait. Available at: https://doi.org/10.1016/j.joca.2021.10.011.   
[6] Google Developers. (2015). Clustering Algorithms | Clustering in Machine Learning. Available at: https://developers.google.com/machine-learning/clustering/clustering-algorithms.   
[7] Rodriguez, M.Z., Comin, C.H., Casanova, D., Bruno, O.M., Amancio, D.R., Costa, L. da F. and Rodrigues, F.A. (2019). Clustering algorithms: A comparative approach. Available at: https://doi.org/10.1371/journal.pone.0210236  
[8] Borkin, M.A., Vo, A.A., Bylinskii, Z., Isola, P., Sunkavalli, S., Oliva, A. and Pfister, H. (2013). What Makes a Visualization Memorable? IEEE Transactions on Visualization and Computer Graphics. Available at https://doi.org/10.1109/TVCG.2013.234  

